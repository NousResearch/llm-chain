"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8175],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var c=a.createContext({}),l=function(e){var t=a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=l(e.components);return a.createElement(c.Provider,{value:t},e.children)},h="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),h=l(n),m=r,d=h["".concat(c,".").concat(m)]||h[m]||u[m]||i;return n?a.createElement(d,o(o({ref:t},p),{},{components:n})):a.createElement(d,o({ref:t},p))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[h]="string"==typeof e?e:r,o[1]=s;for(var l=2;l<i;l++)o[l]=n[l];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},6854:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var a=n(7462),r=(n(7294),n(3905));const i={},o=void 0,s={unversionedId:"chains/sequential-chains",id:"chains/sequential-chains",title:"sequential-chains",description:"Sequential Chains",source:"@site/docs/chains/01-sequential-chains.md",sourceDirName:"chains",slug:"/chains/sequential-chains",permalink:"/docs/chains/sequential-chains",draft:!1,editUrl:"https://github.com/sobelio/llm-chain/tree/main/docs/docs/chains/01-sequential-chains.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"sidebar",previous:{title:"What are LLM chains and why are they useful?",permalink:"/docs/chains/what-are-chains"},next:{title:"Map-Reduce Chains",permalink:"/docs/chains/map-reduce-chains"}},c={},l=[],p={toc:l},h="wrapper";function u(e){let{components:t,...n}=e;return(0,r.kt)(h,(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Sequential Chains"),(0,r.kt)("p",null,"Sequential chains are a convenient way to apply large language models (LLMs) to a sequence of tasks. They connect multiple steps together, where the output of the first step becomes the input of the second step, and so on. This method allows for straightforward processing of information, where each step builds upon the results of the previous one."),(0,r.kt)("p",null,"In this guide, we'll explain how to create and execute a sequential chain using an example. The example demonstrates a two-step process, where the first step generates a personalized birthday email, and the second step summarizes the email into a tweet."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-rust"},'use llm_chain::parameters;\nuse llm_chain::step::Step;\nuse llm_chain::traits::Executor as ExecutorTrait;\nuse llm_chain::{chains::sequential::Chain, prompt};\nuse llm_chain_openai::chatgpt::Executor;\n\n#[tokio::main(flavor = "current_thread")]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Create a new ChatGPT executor with the default settings\n    let exec = Executor::new()?;\n\n    // Create a chain of steps with two prompts\n    let chain: Chain<Executor> = Chain::new(vec![\n        // First step: make a personalized birthday email\n        Step::for_prompt_template(\n            prompt!("You are a bot for making personalized greetings", "Make personalized birthday e-mail to the whole company for {{name}} who has their birthday on {{date}}. Include their name")\n        ),\n\n        // Second step: summarize the email into a tweet. Importantly, the text parameter becomes the result of the previous prompt.\n        Step::for_prompt_template(\n            prompt!( "You are an assistant for managing social media accounts for a company", "Summarize this email into a tweet to be sent by the company, use emoji if you can. \\n--\\n{{text}}")\n        )\n    ]);\n\n    // Run the chain with the provided parameters\n    let res = chain\n        .run(\n            // Create a Parameters object with key-value pairs for the placeholders\n            parameters!("name" => "Emil", "date" => "February 30th 2023"),\n            &exec,\n        )\n        .await\n        .unwrap();\n\n    // Print the result to the console\n    println!("{:?}", res);\n    Ok(())\n}\n')),(0,r.kt)("p",null,"In this example, we start by importing the necessary modules and defining the main function. Then, we create a new ChatGPT executor using the Executor::new() function. The executor is responsible for running the LLM."),(0,r.kt)("p",null,"Next, we create a new Chain object by passing in a vector of Step objects. Each step represents a separate LLM prompt. In this case, we have two steps:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"The first step generates a personalized birthday email using the provided name and date parameters."),(0,r.kt)("li",{parentName:"ol"},"The second step summarizes the previously generated email into a tweet. Note that the {{text}} placeholder in the prompt is automatically filled with the result of the previous step.\nAfter defining the chain, we execute it using the chain.run() method. We provide a Parameters object containing key-value pairs for the placeholders in the prompts (e.g., name and date) and the executor.")),(0,r.kt)("p",null,"Finally, we print the result of the chain to the console."),(0,r.kt)("p",null,"Sequential chains offer an efficient and straightforward way to perform a series of tasks using LLMs. By organizing the steps in a specific order, you can create complex processing pipelines that leverage the capabilities of LLMs effectively."))}u.isMDXComponent=!0}}]);