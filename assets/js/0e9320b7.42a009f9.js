"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8406],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>g});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),m=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},p=function(e){var t=m(e.components);return r.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),c=m(n),d=a,g=c["".concat(s,".").concat(d)]||c[d]||u[d]||o;return n?r.createElement(g,l(l({ref:t},p),{},{components:n})):r.createElement(g,l({ref:t},p))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,l=new Array(o);l[0]=d;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i[c]="string"==typeof e?e:a,l[1]=i;for(var m=2;m<o;m++)l[m]=n[m];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},9049:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>m});var r=n(7462),a=(n(7294),n(3905));const o={slug:"introducing-llm-chain-v080",title:"Introducing LLM-chain v0.8.0 - Expanding the prompt system",authors:["whn"],tags:["llm-chain","update","large language models","rust","tera","templating","prompt system"]},l=void 0,i={permalink:"/blog/introducing-llm-chain-v080",editUrl:"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-26/index.md",source:"@site/blog/2023-04-26/index.md",title:"Introducing LLM-chain v0.8.0 - Expanding the prompt system",description:"We're excited to announce the release of llm-chain v0.8.0, a significant update to our LLM library. This release introduces a host of improvements and new features, including a completely revamped Prompt system and more streamlined handling of Parameters. Let's dive into the details!",date:"2023-04-26T00:00:00.000Z",formattedDate:"April 26, 2023",tags:[{label:"llm-chain",permalink:"/blog/tags/llm-chain"},{label:"update",permalink:"/blog/tags/update"},{label:"large language models",permalink:"/blog/tags/large-language-models"},{label:"rust",permalink:"/blog/tags/rust"},{label:"tera",permalink:"/blog/tags/tera"},{label:"templating",permalink:"/blog/tags/templating"},{label:"prompt system",permalink:"/blog/tags/prompt-system"}],readingTime:1.485,hasTruncateMarker:!1,authors:[{name:"will rudenmalm",title:"making llm-chain",url:"https://github.com/williamhogman",imageURL:"https://github.com/williamhogman.png",key:"whn"}],frontMatter:{slug:"introducing-llm-chain-v080",title:"Introducing LLM-chain v0.8.0 - Expanding the prompt system",authors:["whn"],tags:["llm-chain","update","large language models","rust","tera","templating","prompt system"]},prevItem:{title:"Introducing v0.8.1: Enhanced Prompt Macro and New Conversational Chain Type",permalink:"/blog/2023/04/27/index"},nextItem:{title:"Introducing LLM-chain v0.6.0: Powerful Templating and Improved Prompt System",permalink:"/blog/introducing-llm-chain-v060"}},s={authorsImageUrls:[void 0]},m=[{value:"Revamped Prompt System",id:"revamped-prompt-system",level:2},{value:"Executors No Longer Handle Parameters",id:"executors-no-longer-handle-parameters",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}],p={toc:m},c="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(c,(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"We're excited to announce the release of llm-chain v0.8.0, a significant update to our LLM library. This release introduces a host of improvements and new features, including a completely revamped Prompt system and more streamlined handling of Parameters. Let's dive into the details!"),(0,a.kt)("h2",{id:"revamped-prompt-system"},"Revamped Prompt System"),(0,a.kt)("p",null,"Our new Prompt system has been redesigned from the ground up to provide greater flexibility and efficiency in working with language models. In llm-chain v0.8.0, we've introduced new structs and enums to better represent chat messages and their roles, such as ChatMessage, ChatMessageCollection, and ChatRole. The Data enum has also been introduced to represent either a collection of chat messages or a single text, making it easier to work with different types of data."),(0,a.kt)("p",null,"Furthermore, we've created a more powerful PromptTemplate system that allows you to format prompts with a set of parameters. This enables you to dynamically generate prompts for your language models without the need for cumbersome string manipulation."),(0,a.kt)("h2",{id:"executors-no-longer-handle-parameters"},"Executors No Longer Handle Parameters"),(0,a.kt)("p",null,"With the release of llm-chain v0.8.0, we've shifted the responsibility of handling Parameters from the executors to the main llm-chain crate. This change simplifies the process of working with executors, allowing developers to focus more on the core functionality of their language models."),(0,a.kt)("h2",{id:"whats-next"},"What's Next?"),(0,a.kt)("p",null,"This release marks a significant step forward in the evolution. However, we're not stopping here! We'll continue to refine and expand the capabilities of llm-chain, making it even more powerful and user-friendly."),(0,a.kt)("p",null,"We encourage you to check out llm-chain v0.8.0 and experience the benefits of the improved Prompt system and streamlined handling of Parameters. As always, we appreciate your feedback and contributions to help make llm-chain the best language model library out there."),(0,a.kt)("p",null,"Upgrade to llm-chain v0.8.0 today and take your language models to the next level!"))}u.isMDXComponent=!0}}]);