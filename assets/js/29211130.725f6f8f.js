"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5518],{3905:(e,t,n)=>{n.d(t,{Zo:()=>l,kt:()=>d});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),p=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},l=function(e){var t=p(e.components);return r.createElement(c.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),u=p(n),h=a,d=u["".concat(c,".").concat(h)]||u[h]||m[h]||o;return n?r.createElement(d,i(i({ref:t},l),{},{components:n})):r.createElement(d,i({ref:t},l))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=h;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[u]="string"==typeof e?e:a,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},5140:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var r=n(7462),a=(n(7294),n(3905));const o={},i="Conversational Chains",s={unversionedId:"chains/conversational",id:"chains/conversational",title:"Conversational Chains",description:"Conversational chains enable you to have an ongoing conversation with a large language model (LLM). They keep track of the conversation history and manage the context, ensuring that the LLM's responses remain relevant and coherent throughout the conversation. Conversational chains are particularly useful for chatbot applications, multi-step interactions, and any scenario where context is essential.",source:"@site/docs/chains/03-conversational.md",sourceDirName:"chains",slug:"/chains/conversational",permalink:"/docs/chains/conversational",draft:!1,editUrl:"https://github.com/sobelio/llm-chain/tree/main/docs/docs/chains/03-conversational.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{},sidebar:"sidebar",previous:{title:"Map-Reduce Chains",permalink:"/docs/chains/map-reduce-chains"}},c={},p=[],l={toc:p},u="wrapper";function m(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,r.Z)({},l,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"conversational-chains"},"Conversational Chains"),(0,a.kt)("p",null,"Conversational chains enable you to have an ongoing conversation with a large language model (LLM). They keep track of the conversation history and manage the context, ensuring that the LLM's responses remain relevant and coherent throughout the conversation. Conversational chains are particularly useful for chatbot applications, multi-step interactions, and any scenario where context is essential."),(0,a.kt)("p",null,"In this guide, we'll walk you through an example of a conversational chain with an AI assistant that creates personalized greetings for different individuals. The example demonstrates how to initiate a conversation, send multiple messages, and manage the conversation context."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-rust"},'use llm_chain::{\n    chains::conversation::Chain, executor, output::Output, parameters, prompt, step::Step,\n};\nuse tokio;\n\n#[tokio::main(flavor = "current_thread")]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Create a new ChatGPT executor.\n    let exec = executor!()?;\n\n    // Create a new Chain with the executor.\n    let mut chain = Chain::new(\n        prompt!(system: "You are a robot assistant for making personalized greetings."),\n    )?;\n\n    // Define the conversation steps.\n    let step1 = Step::for_prompt_template(prompt!(user: "Make a personalized greeting for Joe."));\n    let step2 =\n        Step::for_prompt_template(prompt!(user: "Now, create a personalized greeting for Jane."));\n    let step3 = Step::for_prompt_template(\n        prompt!(user: "Finally, create a personalized greeting for Alice."),\n    );\n\n    let step4 = Step::for_prompt_template(prompt!(user: "Remind me who did we just greet."));\n\n    // Execute the conversation steps.\n    let res1 = chain.send_message(step1, &parameters!(), &exec).await?;\n    println!("Step 1: {}", res1.primary_textual_output().await.unwrap());\n\n    let res2 = chain.send_message(step2, &parameters!(), &exec).await?;\n    println!("Step 2: {}", res2.primary_textual_output().await.unwrap());\n\n    let res3 = chain.send_message(step3, &parameters!(), &exec).await?;\n    println!("Step 3: {}", res3.primary_textual_output().await.unwrap());\n\n    let res4 = chain.send_message(step4, &parameters!(), &exec).await?;\n    println!("Step 4: {}", res4.primary_textual_output().await.unwrap());\n\n    Ok(())\n}\n')))}m.isMDXComponent=!0}}]);