"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5243],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>g});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,g=u["".concat(s,".").concat(m)]||u[m]||h[m]||i;return n?a.createElement(g,l(l({ref:t},c),{},{components:n})):a.createElement(g,l({ref:t},c))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=m;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o[u]="string"==typeof e?e:r,l[1]=o;for(var p=2;p<i;p++)l[p]=n[p];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4910:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var a=n(7462),r=(n(7294),n(3905));const i={slug:"using-chatgpt-in-rust",title:"Using ChatGPT in Rust with llm-chain",authors:["whn"],tags:["llm-chain","introduction","chatgpt","rust"]},l="Using ChatGPT in Rust with llm-chain",o={permalink:"/blog/using-chatgpt-in-rust",editUrl:"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-14/index.md",source:"@site/blog/2023-04-14/index.md",title:"Using ChatGPT in Rust with llm-chain",description:"In this blog post, we'll explore how to use ChatGPT in Rust with the help of the llm-chain library. We will walk through a simple example that demonstrates how to generate responses using OpenAI's ChatGPT model.",date:"2023-04-14T00:00:00.000Z",formattedDate:"April 14, 2023",tags:[{label:"llm-chain",permalink:"/blog/tags/llm-chain"},{label:"introduction",permalink:"/blog/tags/introduction"},{label:"chatgpt",permalink:"/blog/tags/chatgpt"},{label:"rust",permalink:"/blog/tags/rust"}],readingTime:1.415,hasTruncateMarker:!1,authors:[{name:"will rudenmalm",title:"making llm-chain",url:"https://github.com/williamhogman",imageURL:"https://github.com/williamhogman.png",key:"whn"}],frontMatter:{slug:"using-chatgpt-in-rust",title:"Using ChatGPT in Rust with llm-chain",authors:["whn"],tags:["llm-chain","introduction","chatgpt","rust"]},prevItem:{title:"Introducing LLM-chain v0.6.0: Powerful Templating and Improved Prompt System",permalink:"/blog/introducing-llm-chain-v060"},nextItem:{title:"Unleashing the Power of Large Language Models with LLM-chain",permalink:"/blog/introducing-llm-chain"}},s={authorsImageUrls:[void 0]},p=[{value:"Getting Started",id:"getting-started",level:2},{value:"Wrapping Up",id:"wrapping-up",level:2}],c={toc:p},u="wrapper";function h(e){let{components:t,...n}=e;return(0,r.kt)(u,(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"In this blog post, we'll explore how to use ChatGPT in Rust with the help of the ",(0,r.kt)("inlineCode",{parentName:"p"},"llm-chain")," library. We will walk through a simple example that demonstrates how to generate responses using OpenAI's ChatGPT model."),(0,r.kt)("h2",{id:"getting-started"},"Getting Started"),(0,r.kt)("p",null,"First, let's start by installing the necessary packages using ",(0,r.kt)("inlineCode",{parentName:"p"},"cargo add"),". You will need the ",(0,r.kt)("inlineCode",{parentName:"p"},"llm-chain")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"llm-chain-openai")," libraries:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"cargo add llm-chain llm-chain-openai\n")),(0,r.kt)("p",null,"Now, let's dive into the code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-rust"},'\nuse llm_chain::{traits::StepExt, Parameters};\nuse llm_chain_openai::chatgpt::{Executor, Model, Role, Step};\n\n#[tokio::main(flavor = "current_thread")]\nasync fn main() {\n    let exec = Executor::new_default();\n    let chain = Step::new(\n        Model::ChatGPT3_5Turbo,\n        [\n            (\n                Role::System,\n                "You are a helpful assistant",\n            ),\n            (Role::User, "Tell me about the Rust programming language"),\n        ],\n    )\n    .to_chain();\n    let res = chain.run(Parameters::new(), &exec).await.unwrap();\n    println!("{:?}", res);\n}\n')),(0,r.kt)("p",null,"In the code snippet above, we begin by importing the necessary modules and functions from the ",(0,r.kt)("inlineCode",{parentName:"p"},"llm-chain")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"llm-chain-openai")," libraries. We then define a simple ",(0,r.kt)("inlineCode",{parentName:"p"},"main")," function that uses the ",(0,r.kt)("inlineCode",{parentName:"p"},"Executor")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"Step")," structs to create a conversational chain."),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"Model::ChatGPT3_5Turbo")," model is used as the language model in this example. We also define two steps in the conversation: the first one sets the role of the assistant and the second one asks a question about the Rust programming language."),(0,r.kt)("p",null,"Finally, we execute the conversation chain using the ",(0,r.kt)("inlineCode",{parentName:"p"},"run")," method and print the generated response."),(0,r.kt)("h2",{id:"wrapping-up"},"Wrapping Up"),(0,r.kt)("p",null,"As you can see, using ChatGPT in Rust with ",(0,r.kt)("inlineCode",{parentName:"p"},"llm-chain")," is a straightforward and efficient process. The library makes it easy to build and manage conversational agents in Rust, allowing developers to focus on creating more powerful and interactive applications."),(0,r.kt)("p",null,"To continue learning about ChatGPT in Rust and how to make the most of the ",(0,r.kt)("inlineCode",{parentName:"p"},"llm-chain")," library, try our ",(0,r.kt)("a",{parentName:"p",href:"https://chat.openai.com/docs/getting-started-tutorial/index"},"tutorial")," ."))}h.isMDXComponent=!0}}]);