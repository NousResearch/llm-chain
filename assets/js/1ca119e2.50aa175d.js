"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7703],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>g});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),p=c(n),h=r,g=p["".concat(s,".").concat(h)]||p[h]||m[h]||i;return n?a.createElement(g,o(o({ref:t},u),{},{components:n})):a.createElement(g,o({ref:t},u))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[p]="string"==typeof e?e:r,o[1]=l;for(var c=2;c<i;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},3628:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var a=n(7462),r=(n(7294),n(3905));const i={slug:"introducing-llm-chain",title:"Unleashing the Power of Large Language Models with LLM-chain",authors:["whn"],tags:["llm-chain","introduction","large language models","rust"]},o=void 0,l={permalink:"/blog/introducing-llm-chain",editUrl:"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-10/index.md",source:"@site/blog/2023-04-10/index.md",title:"Unleashing the Power of Large Language Models with LLM-chain",description:"We're excited to announce the release of LLM-chain, a Rust library designed to help developers work with Large Language Models (LLMs) more effectively. Our primary focus is on providing robust support for prompt templates and chaining together prompts in multi-step chains, enabling complex tasks that LLMs can't handle in a single step. This includes, but is not limited to, summarizing lengthy texts or performing advanced data processing tasks.",date:"2023-04-10T00:00:00.000Z",formattedDate:"April 10, 2023",tags:[{label:"llm-chain",permalink:"/blog/tags/llm-chain"},{label:"introduction",permalink:"/blog/tags/introduction"},{label:"large language models",permalink:"/blog/tags/large-language-models"},{label:"rust",permalink:"/blog/tags/rust"}],readingTime:1.39,hasTruncateMarker:!1,authors:[{name:"will rudenmalm",title:"making llm-chain",url:"https://github.com/williamhogman",imageURL:"https://github.com/williamhogman.png",key:"whn"}],frontMatter:{slug:"introducing-llm-chain",title:"Unleashing the Power of Large Language Models with LLM-chain",authors:["whn"],tags:["llm-chain","introduction","large language models","rust"]},prevItem:{title:"Using ChatGPT in Rust with llm-chain",permalink:"/blog/using-chatgpt-in-rust"}},s={authorsImageUrls:[void 0]},c=[{value:"Features of LLM-chain",id:"features-of-llm-chain",level:2},{value:"Connect with Us",id:"connect-with-us",level:2},{value:"Getting Started with LLM-chain",id:"getting-started-with-llm-chain",level:2}],u={toc:c},p="wrapper";function m(e){let{components:t,...n}=e;return(0,r.kt)(p,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"We're excited to announce the release of LLM-chain, a Rust library designed to help developers work with Large Language Models (LLMs) more effectively. Our primary focus is on providing robust support for prompt templates and chaining together prompts in multi-step chains, enabling complex tasks that LLMs can't handle in a single step. This includes, but is not limited to, summarizing lengthy texts or performing advanced data processing tasks."),(0,r.kt)("h2",{id:"features-of-llm-chain"},"Features of LLM-chain"),(0,r.kt)("p",null,"LLM-chain comes with a variety of features that make it easier to work with LLMs, including:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Prompt templates"),": Create reusable and easily customizable prompt templates for consistent and structured interactions with LLMs."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Chains"),": Build powerful chains of prompts that allow you to execute more complex tasks, step by step, leveraging the full potential of LLMs."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"ChatGPT support"),": Currently supports ChatGPT models, with plans to add support for more LLMs in the future, such as LLaMa and Stanford's Alpaca models."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Tools"),": Enhance your AI agents' capabilities by giving them access to various tools, such as running Bash commands, executing Python scripts, or performing web searches, enabling more complex and powerful interactions."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Extensibility"),": Designed with extensibility in mind, making it easy to integrate additional LLMs as the ecosystem grows and new models are developed."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Community-driven"),": We welcome and encourage contributions from the community to help improve and expand the capabilities of LLM-chain.")),(0,r.kt)("h2",{id:"connect-with-us"},"Connect with Us"),(0,r.kt)("p",null,"If you have any questions, suggestions, or feedback, feel free to join our ",(0,r.kt)("a",{parentName:"p",href:"https://discord.gg/kewN9Gtjt2"},"Discord community"),". We're always excited to hear from our users and learn about your experiences with LLM-chain."),(0,r.kt)("h2",{id:"getting-started-with-llm-chain"},"Getting Started with LLM-chain"),(0,r.kt)("p",null,"Check out our ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/sobelio/llm-chain"},"Github repository")," or the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.rs/llm-chain"},"documentation")," to get started."))}m.isMDXComponent=!0}}]);