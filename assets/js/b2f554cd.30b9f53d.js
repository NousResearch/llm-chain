"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2023/04/28/parsing-llm-input-with-llm-chain-0-8-2","metadata":{"permalink":"/blog/2023/04/28/parsing-llm-input-with-llm-chain-0-8-2","editUrl":"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-28/parsing-llm-input-with-llm-chain-0-8-2.md","source":"@site/blog/2023-04-28/parsing-llm-input-with-llm-chain-0-8-2.md","title":"Announcement: LLM Chain Update 0.8.2","description":"We are excited to announce the release of LLM Chain version 0.8.2! This update introduces some important improvements to our library, making it even more powerful and easy to use.","date":"2023-04-28T00:00:00.000Z","formattedDate":"April 28, 2023","tags":[],"readingTime":0.85,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"nextItem":{"title":"Introducing v0.8.1: Enhanced Prompt Macro and New Conversational Chain Type","permalink":"/blog/2023/04/27/index"}},"content":"We are excited to announce the release of LLM Chain version 0.8.2! This update introduces some important improvements to our library, making it even more powerful and easy to use.\\n\\n## What\'s new in 0.8.2\\n\\n### New `extract_labeled_text` function\\n\\nWe\'ve added a new function called `extract_labeled_text` in the parsing module. This function is designed to help you parse labeled text that is often generated by LLMs (Language Learning Machines). LLMs typically generate text like this:\\n\\n```markdown\\n- *foo*: bar\\n- hello: world\\n```\\n\\n### Improved find_yaml function\\nIn this update, we have also improved the find_yaml function. It now returns the results in the order they appear in the document, making it more consistent and easier to work with.\\n\\n## Get started with 0.8.2\\n\\nTo start using LLM Chain version 0.8.2, update your dependency in your Cargo.toml file:\\n\\n```toml\\nllm_chain = \\"0.8.2\\"\\n```\\n\\nWe hope you enjoy these new features and improvements! As always, if you have any questions or feedback, please feel free to reach out to our team."},{"id":"/2023/04/27/index","metadata":{"permalink":"/blog/2023/04/27/index","editUrl":"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-27/index.md","source":"@site/blog/2023-04-27/index.md","title":"Introducing v0.8.1: Enhanced Prompt Macro and New Conversational Chain Type","description":"We are excited to announce the release of version 0.8.1, which brings two major improvements to our Large Language Model (LLM) library: an enhanced prompt! macro and a new Conversational chain type. These updates make it even easier for developers to create rich and interactive applications powered by LLMs.","date":"2023-04-27T00:00:00.000Z","formattedDate":"April 27, 2023","tags":[],"readingTime":2.305,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"Announcement: LLM Chain Update 0.8.2","permalink":"/blog/2023/04/28/parsing-llm-input-with-llm-chain-0-8-2"},"nextItem":{"title":"Introducing LLM-chain v0.8.0 - Expanding the prompt system","permalink":"/blog/introducing-llm-chain-v080"}},"content":"We are excited to announce the release of version 0.8.1, which brings two major improvements to our Large Language Model (LLM) library: an enhanced `prompt!` macro and a new Conversational chain type. These updates make it even easier for developers to create rich and interactive applications powered by LLMs.\\n\\n## Enhanced Prompt Macro with Prefixes\\n\\nThe `prompt!` macro has been updated to support prefixes, making it more expressive and convenient to use. With this new feature, you can now create chat prompts by simply prefixing them with `user:`, `assistant:`, or `system:`. Here\'s an example of how to use the new syntax:\\n\\n```rust\\n\\nlet user_prompt = prompt!(user: \\"Hello, Mr Bot, help me figure out what to do next\\");\\nlet system_prompt = prompt!(system: \\"You are a clever assistant that\\");\\n```\\n\\nBy using these prefixes, you can create more complex and interactive prompts for various use cases, such as building chatbots, automating tasks, or generating text.\\n\\n## New Conversational Chain Type\\n\\nWe\'re also introducing the Conversational chain type, which enables you to have ongoing conversations with LLMs. Conversational chains manage the conversation history and context, ensuring that the LLM\'s responses remain relevant and coherent throughout the interaction. This new chain type is particularly useful for chatbot applications, multi-step interactions, and any scenario where context is essential.\\n\\nHere\'s a quick example of a Conversational chain:\\n\\n```rust\\nuse llm_chain::{\\n    chains::conversation::Chain, executor, output::Output, parameters, prompt, step::Step,\\n};\\nuse tokio;\\n\\n#[tokio::main(flavor = \\"current_thread\\")]\\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\\n    // Create a new ChatGPT executor.\\n    let exec = executor!()?;\\n\\n    // Create a new Chain with the executor.\\n    let mut chain = Chain::new(\\n        prompt!(system: \\"You are a robot assistant for making personalized greetings.\\"),\\n    )?;\\n\\n    // Define the conversation steps.\\n    let step1 = Step::for_prompt_template(prompt!(user: \\"Make a personalized greeting for Joe.\\"));\\n    let step2 =\\n        Step::for_prompt_template(prompt!(user: \\"Now, create a personalized greeting for Jane.\\"));\\n    let step3 = Step::for_prompt_template(\\n        prompt!(user: \\"Finally, create a personalized greeting for Alice.\\"),\\n    );\\n\\n    let step4 = Step::for_prompt_template(prompt!(user: \\"Remind me who did we just greet.\\"));\\n\\n    // Execute the conversation steps.\\n    let res1 = chain.send_message(step1, &parameters!(), &exec).await?;\\n    println!(\\"Step 1: {}\\", res1.primary_textual_output().await.unwrap());\\n\\n    let res2 = chain.send_message(step2, &parameters!(), &exec).await?;\\n    println!(\\"Step 2: {}\\", res2.primary_textual_output().await.unwrap());\\n\\n    let res3 = chain.send_message(step3, &parameters!(), &exec).await?;\\n    println!(\\"Step 3: {}\\", res3.primary_textual_output().await.unwrap());\\n\\n    let res4 = chain.send_message(step4, &parameters!(), &exec).await?;\\n    println!(\\"Step 4: {}\\", res4.primary_textual_output().await.unwrap());\\n\\n    Ok(())\\n}\\n```\\n\\nWith the Conversational chain, you can now easily send multiple messages and manage the conversation context without having to worry about manual context management.\\n\\n## Upgrade Today\\n\\nWe encourage you to upgrade to version 0.8.1 and take advantage of these new features. The enhanced `prompt!` macro and the new Conversational chain type will make your LLM-powered applications even more interactive and engaging.\\n\\nAs always, we appreciate your feedback and suggestions. Feel free to reach out to our team for any questions or concerns. Happy coding!"},{"id":"introducing-llm-chain-v080","metadata":{"permalink":"/blog/introducing-llm-chain-v080","editUrl":"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-26/index.md","source":"@site/blog/2023-04-26/index.md","title":"Introducing LLM-chain v0.8.0 - Expanding the prompt system","description":"We\'re excited to announce the release of llm-chain v0.8.0, a significant update to our LLM library. This release introduces a host of improvements and new features, including a completely revamped Prompt system and more streamlined handling of Parameters. Let\'s dive into the details!","date":"2023-04-26T00:00:00.000Z","formattedDate":"April 26, 2023","tags":[{"label":"llm-chain","permalink":"/blog/tags/llm-chain"},{"label":"update","permalink":"/blog/tags/update"},{"label":"large language models","permalink":"/blog/tags/large-language-models"},{"label":"rust","permalink":"/blog/tags/rust"},{"label":"tera","permalink":"/blog/tags/tera"},{"label":"templating","permalink":"/blog/tags/templating"},{"label":"prompt system","permalink":"/blog/tags/prompt-system"}],"readingTime":1.485,"hasTruncateMarker":false,"authors":[{"name":"will rudenmalm","title":"making llm-chain","url":"https://github.com/williamhogman","imageURL":"https://github.com/williamhogman.png","key":"whn"}],"frontMatter":{"slug":"introducing-llm-chain-v080","title":"Introducing LLM-chain v0.8.0 - Expanding the prompt system","authors":["whn"],"tags":["llm-chain","update","large language models","rust","tera","templating","prompt system"]},"prevItem":{"title":"Introducing v0.8.1: Enhanced Prompt Macro and New Conversational Chain Type","permalink":"/blog/2023/04/27/index"},"nextItem":{"title":"Introducing LLM-chain v0.6.0: Powerful Templating and Improved Prompt System","permalink":"/blog/introducing-llm-chain-v060"}},"content":"We\'re excited to announce the release of llm-chain v0.8.0, a significant update to our LLM library. This release introduces a host of improvements and new features, including a completely revamped Prompt system and more streamlined handling of Parameters. Let\'s dive into the details!\\n\\n## Revamped Prompt System\\n\\nOur new Prompt system has been redesigned from the ground up to provide greater flexibility and efficiency in working with language models. In llm-chain v0.8.0, we\'ve introduced new structs and enums to better represent chat messages and their roles, such as ChatMessage, ChatMessageCollection, and ChatRole. The Data enum has also been introduced to represent either a collection of chat messages or a single text, making it easier to work with different types of data.\\n\\nFurthermore, we\'ve created a more powerful PromptTemplate system that allows you to format prompts with a set of parameters. This enables you to dynamically generate prompts for your language models without the need for cumbersome string manipulation.\\n\\n## Executors No Longer Handle Parameters\\n\\nWith the release of llm-chain v0.8.0, we\'ve shifted the responsibility of handling Parameters from the executors to the main llm-chain crate. This change simplifies the process of working with executors, allowing developers to focus more on the core functionality of their language models.\\n\\n## What\'s Next?\\n\\nThis release marks a significant step forward in the evolution. However, we\'re not stopping here! We\'ll continue to refine and expand the capabilities of llm-chain, making it even more powerful and user-friendly.\\n\\nWe encourage you to check out llm-chain v0.8.0 and experience the benefits of the improved Prompt system and streamlined handling of Parameters. As always, we appreciate your feedback and contributions to help make llm-chain the best language model library out there.\\n\\nUpgrade to llm-chain v0.8.0 today and take your language models to the next level!"},{"id":"introducing-llm-chain-v060","metadata":{"permalink":"/blog/introducing-llm-chain-v060","editUrl":"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-17/index.md","source":"@site/blog/2023-04-17/index.md","title":"Introducing LLM-chain v0.6.0: Powerful Templating and Improved Prompt System","description":"We are thrilled to announce the release of llm-chain v0.6.0, which introduces significant enhancements to our library. This update focuses on making the llm-chain more robust and versatile, allowing developers to build even more advanced applications with ease.","date":"2023-04-17T00:00:00.000Z","formattedDate":"April 17, 2023","tags":[{"label":"llm-chain","permalink":"/blog/tags/llm-chain"},{"label":"update","permalink":"/blog/tags/update"},{"label":"large language models","permalink":"/blog/tags/large-language-models"},{"label":"rust","permalink":"/blog/tags/rust"},{"label":"tera","permalink":"/blog/tags/tera"},{"label":"templating","permalink":"/blog/tags/templating"},{"label":"prompt system","permalink":"/blog/tags/prompt-system"}],"readingTime":1.695,"hasTruncateMarker":false,"authors":[{"name":"will rudenmalm","title":"making llm-chain","url":"https://github.com/williamhogman","imageURL":"https://github.com/williamhogman.png","key":"whn"}],"frontMatter":{"slug":"introducing-llm-chain-v060","title":"Introducing LLM-chain v0.6.0: Powerful Templating and Improved Prompt System","authors":["whn"],"tags":["llm-chain","update","large language models","rust","tera","templating","prompt system"]},"prevItem":{"title":"Introducing LLM-chain v0.8.0 - Expanding the prompt system","permalink":"/blog/introducing-llm-chain-v080"},"nextItem":{"title":"Using ChatGPT in Rust with llm-chain","permalink":"/blog/using-chatgpt-in-rust"}},"content":"We are thrilled to announce the release of llm-chain v0.6.0, which introduces significant enhancements to our library. This update focuses on making the llm-chain more robust and versatile, allowing developers to build even more advanced applications with ease.\\n\\n### Major updates\\n\\n#### 1. The switch to the `tera` template language\\n\\nOne of the most significant changes in this release is the introduction of the `tera` template language. This powerful and flexible templating system enables developers to create dynamic and complex templates for their projects. The `tera` language allows for more advanced control structures and filters, making it a substantial upgrade from the previous templating system.\\n\\n#### 2. Improved prompt system\\n\\nAnother notable update is the revamped prompt system. With llm-chain v0.6.0, the prompt system now supports both Chat and completion-style models. This improvement means developers no longer need to worry about whether they are using a completion or chat model when crafting prompts. This unified approach simplifies the development process and makes it easier to work with various types of language models.\\n\\n#### 3. Updated LLaMA.cpp\\n\\nThe latest version of LLaMA.cpp has been integrated into this release, ensuring better performance and stability for your projects.\\n\\n### Other improvements\\n\\n#### 1. Safer error handling\\n\\nIn addition to the major updates, llm-chain v0.6.0 also brings improvements to error handling. Templates now return `Result` rather than panicking on errors, making it more convenient to handle any issues that may arise during development. Similarly, Executors also return `Result` instead of panicking on errors, providing a more consistent and safer API.\\n\\n### Time to move on from the old templating system\\n\\nWith the introduction of the `tera` template language, we strongly recommend moving away from the old templating system. This update provides a solid foundation for building even more advanced applications using the llm-chain library.\\n\\nWe hope you\'re as excited about these enhancements as we are! As always, we appreciate your feedback and support. If you have any questions or need help, please don\'t hesitate to reach out on [Discord](https://discord.gg/kewN9Gtjt2) !\\n\\nHappy coding! \ud83d\ude80"},{"id":"using-chatgpt-in-rust","metadata":{"permalink":"/blog/using-chatgpt-in-rust","editUrl":"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-14/index.md","source":"@site/blog/2023-04-14/index.md","title":"Using ChatGPT in Rust with llm-chain","description":"In this blog post, we\'ll explore how to use ChatGPT in Rust with the help of the llm-chain library. We will walk through a simple example that demonstrates how to generate responses using OpenAI\'s ChatGPT model.","date":"2023-04-14T00:00:00.000Z","formattedDate":"April 14, 2023","tags":[{"label":"llm-chain","permalink":"/blog/tags/llm-chain"},{"label":"introduction","permalink":"/blog/tags/introduction"},{"label":"chatgpt","permalink":"/blog/tags/chatgpt"},{"label":"rust","permalink":"/blog/tags/rust"}],"readingTime":1.415,"hasTruncateMarker":false,"authors":[{"name":"will rudenmalm","title":"making llm-chain","url":"https://github.com/williamhogman","imageURL":"https://github.com/williamhogman.png","key":"whn"}],"frontMatter":{"slug":"using-chatgpt-in-rust","title":"Using ChatGPT in Rust with llm-chain","authors":["whn"],"tags":["llm-chain","introduction","chatgpt","rust"]},"prevItem":{"title":"Introducing LLM-chain v0.6.0: Powerful Templating and Improved Prompt System","permalink":"/blog/introducing-llm-chain-v060"},"nextItem":{"title":"Unleashing the Power of Large Language Models with LLM-chain","permalink":"/blog/introducing-llm-chain"}},"content":"In this blog post, we\'ll explore how to use ChatGPT in Rust with the help of the `llm-chain` library. We will walk through a simple example that demonstrates how to generate responses using OpenAI\'s ChatGPT model.\\n\\n## Getting Started\\n\\nFirst, let\'s start by installing the necessary packages using `cargo add`. You will need the `llm-chain` and `llm-chain-openai` libraries:\\n\\n```sh\\ncargo add llm-chain llm-chain-openai\\n```\\n\\nNow, let\'s dive into the code:\\n\\n```rust\\n\\nuse llm_chain::{traits::StepExt, Parameters};\\nuse llm_chain_openai::chatgpt::{Executor, Model, Role, Step};\\n\\n#[tokio::main(flavor = \\"current_thread\\")]\\nasync fn main() {\\n    let exec = Executor::new_default();\\n    let chain = Step::new(\\n        Model::ChatGPT3_5Turbo,\\n        [\\n            (\\n                Role::System,\\n                \\"You are a helpful assistant\\",\\n            ),\\n            (Role::User, \\"Tell me about the Rust programming language\\"),\\n        ],\\n    )\\n    .to_chain();\\n    let res = chain.run(Parameters::new(), &exec).await.unwrap();\\n    println!(\\"{:?}\\", res);\\n}\\n```\\n\\nIn the code snippet above, we begin by importing the necessary modules and functions from the `llm-chain` and `llm-chain-openai` libraries. We then define a simple `main` function that uses the `Executor` and `Step` structs to create a conversational chain.\\n\\nThe `Model::ChatGPT3_5Turbo` model is used as the language model in this example. We also define two steps in the conversation: the first one sets the role of the assistant and the second one asks a question about the Rust programming language.\\n\\nFinally, we execute the conversation chain using the `run` method and print the generated response.\\n\\n## Wrapping Up\\n\\nAs you can see, using ChatGPT in Rust with `llm-chain` is a straightforward and efficient process. The library makes it easy to build and manage conversational agents in Rust, allowing developers to focus on creating more powerful and interactive applications.\\n\\nTo continue learning about ChatGPT in Rust and how to make the most of the `llm-chain` library, try our [tutorial](https://chat.openai.com/docs/getting-started-tutorial/index) ."},{"id":"introducing-llm-chain","metadata":{"permalink":"/blog/introducing-llm-chain","editUrl":"https://github.com/sobelio/llm-chain/tree/main/blog/blog/2023-04-10/index.md","source":"@site/blog/2023-04-10/index.md","title":"Unleashing the Power of Large Language Models with LLM-chain","description":"We\'re excited to announce the release of LLM-chain, a Rust library designed to help developers work with Large Language Models (LLMs) more effectively. Our primary focus is on providing robust support for prompt templates and chaining together prompts in multi-step chains, enabling complex tasks that LLMs can\'t handle in a single step. This includes, but is not limited to, summarizing lengthy texts or performing advanced data processing tasks.","date":"2023-04-10T00:00:00.000Z","formattedDate":"April 10, 2023","tags":[{"label":"llm-chain","permalink":"/blog/tags/llm-chain"},{"label":"introduction","permalink":"/blog/tags/introduction"},{"label":"large language models","permalink":"/blog/tags/large-language-models"},{"label":"rust","permalink":"/blog/tags/rust"}],"readingTime":1.39,"hasTruncateMarker":false,"authors":[{"name":"will rudenmalm","title":"making llm-chain","url":"https://github.com/williamhogman","imageURL":"https://github.com/williamhogman.png","key":"whn"}],"frontMatter":{"slug":"introducing-llm-chain","title":"Unleashing the Power of Large Language Models with LLM-chain","authors":["whn"],"tags":["llm-chain","introduction","large language models","rust"]},"prevItem":{"title":"Using ChatGPT in Rust with llm-chain","permalink":"/blog/using-chatgpt-in-rust"}},"content":"We\'re excited to announce the release of LLM-chain, a Rust library designed to help developers work with Large Language Models (LLMs) more effectively. Our primary focus is on providing robust support for prompt templates and chaining together prompts in multi-step chains, enabling complex tasks that LLMs can\'t handle in a single step. This includes, but is not limited to, summarizing lengthy texts or performing advanced data processing tasks.\\n\\n## Features of LLM-chain\\n\\nLLM-chain comes with a variety of features that make it easier to work with LLMs, including:\\n\\n- **Prompt templates**: Create reusable and easily customizable prompt templates for consistent and structured interactions with LLMs.\\n- **Chains**: Build powerful chains of prompts that allow you to execute more complex tasks, step by step, leveraging the full potential of LLMs.\\n- **ChatGPT support**: Currently supports ChatGPT models, with plans to add support for more LLMs in the future, such as LLaMa and Stanford\'s Alpaca models.\\n- **Tools**: Enhance your AI agents\' capabilities by giving them access to various tools, such as running Bash commands, executing Python scripts, or performing web searches, enabling more complex and powerful interactions.\\n- **Extensibility**: Designed with extensibility in mind, making it easy to integrate additional LLMs as the ecosystem grows and new models are developed.\\n- **Community-driven**: We welcome and encourage contributions from the community to help improve and expand the capabilities of LLM-chain.\\n\\n## Connect with Us\\n\\nIf you have any questions, suggestions, or feedback, feel free to join our [Discord community](https://discord.gg/kewN9Gtjt2). We\'re always excited to hear from our users and learn about your experiences with LLM-chain.\\n\\n## Getting Started with LLM-chain\\n\\nCheck out our [Github repository](https://github.com/sobelio/llm-chain) or the [documentation](https://docs.rs/llm-chain) to get started."}]}')}}]);